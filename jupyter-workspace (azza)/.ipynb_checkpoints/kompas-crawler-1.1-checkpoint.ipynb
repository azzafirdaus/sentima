{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanggal mulai (yyyy-mm-dd): 2016-1-1\n",
      "tanggal akhir (yyyy-mm-dd): 2016-1-1\n",
      "nama file: tes-tanggal\n",
      "selesai 1 hari\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import timedelta, date, datetime\n",
    "import urllib.request, urllib.parse, re, json, sys\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)+1):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "#bikin class crawler dengan parameter inputnya adalah url\n",
    "def kompas_crawler(myurl, namaFile = None):\n",
    "    #bikin list buat hasil pencarian\n",
    "    try:\n",
    "        urllib.request.urlopen(myurl)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    #bikin soup buat parsing html dari url\n",
    "    soup = BeautifulSoup((urllib.request.urlopen(myurl)).read(), \"lxml\")\n",
    "\n",
    "    daftar = []\n",
    "    documents = []\n",
    "    \n",
    "    #url page\n",
    "    pages = soup.select('a[href*=\"search/news\"]')\n",
    "    for each_page in pages:\n",
    "        try:\n",
    "            urllib.request.urlopen(each_page.get('href'))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        page_soup = BeautifulSoup((urllib.request.urlopen(each_page.get('href'))).read(), \"lxml\")\n",
    "        \n",
    "        page_documents = [x.a['href'] for x in page_soup.find_all('div', attrs={'class' : 'list-latest'}) if (\"megapolitan\" in x.a['href']) or (\"nasional\" in x.a['href'])]\n",
    "        documents.extend(page_documents)\n",
    "    \n",
    "    for doc in documents:\n",
    "        try:\n",
    "            urllib.request.urlopen(doc+\"?page=all\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        #bikin soup buat parsing halaman masing-masing berita\n",
    "        innerSoup = BeautifulSoup((urllib.request.urlopen(doc+\"?page=all\")).read(), \"lxml\")\n",
    "        \n",
    "        judul = innerSoup.find('title').get_text()\n",
    "        konten = innerSoup.find(\"div\", {\"class\": \"kcm-read-text\"}).get_text(\" \", strip=True)\n",
    "        #lokasi = cap(konten[0:(konten.find('komp')-1)], 20)\n",
    "\n",
    "        #bikin dict buat nyimpen data 'Judul' dan 'Konten' suatu berita\n",
    "        dictBerita = {'Konten': konten, 'Judul': judul}\n",
    "\n",
    "        #masukin dict berita tersebut ke dalam list daftar\n",
    "        daftar.append(dictBerita)\n",
    "\n",
    "    global iterasi\n",
    "    print(\"selesai \"+ str(iterasi)+ \" hari\")\n",
    "    iterasi += 1\n",
    "    \n",
    "    return daftar\n",
    "    \n",
    "if __name__ == \"__main__\":    \n",
    "\n",
    "#     try:\n",
    "#         datetime.strptime(sys.argv[1], '%Y-%m-%d')\n",
    "#         datetime.strptime(sys.argv[2], '%Y-%m-%d')\n",
    "#     except ValueError:\n",
    "#         print(\"python kompas_crawler.py 'yyyy-mm-dd' 'yyyy-mm-dd'\")\n",
    "    \n",
    "#     start_date = datetime.strptime(sys.argv[1], '%Y-%m-%d').date()\n",
    "#     end_date = datetime.strptime(sys.argv[2], '%Y-%m-%d').date()\n",
    "    \n",
    "    start = input(\"tanggal mulai (yyyy-mm-dd): \")\n",
    "    end = input(\"tanggal akhir (yyyy-mm-dd): \")\n",
    "    file = input(\"nama file untuk menyimpan: \")\n",
    "    \n",
    "    start_date = datetime.strptime(start, '%Y-%m-%d').date()\n",
    "    end_date = datetime.strptime(end, '%Y-%m-%d').date()\n",
    "    \n",
    "    urllib.parse.quote(':')\n",
    "    iterasi = 1\n",
    "    docs = []\n",
    "\n",
    "    Url = \"http://megapolitan.kompas.com/search/news/\"\n",
    "\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        docs.extend(kompas_crawler(Url+single_date.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "    with open(file+'.json', 'w+') as output:\n",
    "        json.dump(docs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
